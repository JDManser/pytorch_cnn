{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1088 2048]\n",
      "[1088 2048]\n"
     ]
    }
   ],
   "source": [
    "#THIS IS JUST TO GET LIST OF CHUNKED ARRAYS\n",
    "\n",
    "\n",
    "def get_proportion_of_background_pixels_in_chunk(chunk):\n",
    "    chunk = np.array(chunk)\n",
    "\n",
    "    pixels = chunk.flatten()\n",
    "\n",
    "    # How many pixels in the chunk have an intensity of 0?\n",
    "    # (We will count them as background pixels)\n",
    "    background_pixel_count = (pixels==0).sum()\n",
    "\n",
    "    return background_pixel_count / len(pixels)\n",
    "\n",
    "def split_hsi_into_chunks(hyperspectral_image, chunk_size):\n",
    "\n",
    "    chunks = []\n",
    "    # Get the width and height for the image    \n",
    "    image_size = np.array(list(hyperspectral_image.shape[:-1]))\n",
    "    print(image_size)\n",
    "\n",
    "    # Get how many chunks are needed for the image\n",
    "    # as an array of integers\n",
    "    image_chunks = np.ceil(image_size / chunk_size).astype(np.int64)\n",
    "\n",
    "    for x in range(image_chunks[0]):\n",
    "        for y in range(image_chunks[1]):\n",
    "\n",
    "            # Get the pixels which should be covered by the chunk.\n",
    "            chunk_start = np.array([x,y]) * chunk_size\n",
    "            chunk_start = np.minimum(chunk_start, image_size)\n",
    "            chunk_end = np.minimum(chunk_start + chunk_size, image_size)\n",
    "\n",
    "            image_chunk = hyperspectral_image[chunk_start[0]:chunk_end[0], chunk_start[1]:chunk_end[1], :]\n",
    "\n",
    "            chunks.append(image_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Remove all chunks in a list of chunks which have higher than\n",
    "# background_percentage percentage of background pixels.\n",
    "def remove_background_chunks_by_percentage(chunks, background_percentage):\n",
    "\n",
    "    threshold = (1 - background_percentage)\n",
    "    filtered_chunks = filter((lambda x : get_proportion_of_background_pixels_in_chunk(x) <= threshold), chunks)\n",
    "    return np.array(list(filtered_chunks))\n",
    "\n",
    "\n",
    "data_path_1 = np.load('C:\\\\Users\\\\bendo\\\\Desktop\\\\uni\\\\ProjectA\\\\sugarcane\\\\TEST\\\\con_pro\\\\Q78_1_c_irradiance.npy')\n",
    "data_path_2 = np.load('C:\\\\Users\\\\bendo\\\\Desktop\\\\uni\\\\ProjectA\\\\sugarcane\\\\TEST\\\\dis_pro\\\\Q78_1_irradiance.npy') \n",
    "\n",
    "chunks_c = split_hsi_into_chunks(data_path_1, 32)\n",
    "chunks_d = split_hsi_into_chunks(data_path_2, 32)\n",
    "\n",
    "# Remove all chunks with more than 90% background content\n",
    "chunks_c = remove_background_chunks_by_percentage(chunks_c, 0.1)\n",
    "chunks_d = remove_background_chunks_by_percentage(chunks_d, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list:  537 chunks_list 537\n"
     ]
    }
   ],
   "source": [
    "joint_chunks = np.concatenate((chunks_c,chunks_d))\n",
    "\n",
    "c_l = []\n",
    "d_l = []\n",
    "\n",
    "#need to figure this out, just away to get list of classes for now\n",
    "\n",
    "classes = ('contorl', 'disease')\n",
    "\n",
    "for x in range(len(chunks_c)):\n",
    "    c_l.append([0])\n",
    "\n",
    "for x in range(len(chunks_d)):\n",
    "    d_l.append([1])\n",
    "\n",
    "classes_lists = c_l + d_l\n",
    "\n",
    "print('class list: ',len(classes_lists), 'chunks_list', len(joint_chunks)) #should be same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create x and y for data\n",
    "\n",
    "x = torch.from_numpy(joint_chunks)\n",
    "y = torch.FloatTensor(classes_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape x for pytorch\n",
    "#WARNING THIS COULD BE WRONG\n",
    "\n",
    "x = torch.reshape(x,(len(joint_chunks),161,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(429, 108, 429, 108)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training and test 80/20 split\n",
    "#also shuffle\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into dataloader for pytroch\n",
    "\n",
    "#code from >> https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "my_dataset = TensorDataset(X_train,y_train)\n",
    "my_dataloader = DataLoader(my_dataset) # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(161, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(my_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = TensorDataset(X_test,y_test)\n",
    "my_dataset_test = DataLoader(my_dataset)\n",
    "dataiter = iter(my_dataset_test)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m5s\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[143], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses[\u001b[43mpredicted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 67 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in my_dataset_test:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
